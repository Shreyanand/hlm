{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpolCI6RYQV3"
      },
      "source": [
        "# History of Language Models: Evolution, Frameworks, Deployment\n",
        "\n",
        "\n",
        "## Naive Language Model (N-gram)\n",
        "\n",
        "**Concept:**\n",
        "- **N-gram Models**: These models use sequences of 'N' words to predict the next word in a sentence. A bigram model considers pairs of words, while a trigram model considers triplets.\n",
        "- **Bigram**: Uses pairs of words, e.g., \"natural language\", \"language processing\".\n",
        "- **Trigram**: Uses triplets of words, e.g., \"natural language processing\".\n",
        "- **Limitations**: Limited context as it only considers fixed-size word groups and cannot capture long-range dependencies or semantic meaning.\n",
        "\n",
        "**Application**:\n",
        "- Predicts the next word based on the previous words in the sequence. For instance, after \"natural language\", a bigram model predicts \"processing\" if it's seen frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAH7a5xZECSG",
        "outputId": "53b4654b-2c83-4f9c-c760-5a6df8ebdd26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram Prediction for 'natural': language\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "\n",
        "# Example text\n",
        "text = \"I love natural language processing because natural language processing is fun\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = text.split()\n",
        "\n",
        "# Generate bigrams and trigrams\n",
        "bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "trigrams = [(words[i], words[i+1], words[i+2]) for i in range(len(words)-2)]\n",
        "\n",
        "# Count bigram and trigram frequencies\n",
        "bigram_freq = defaultdict(Counter)\n",
        "trigram_freq = defaultdict(Counter)\n",
        "\n",
        "for bigram in bigrams:\n",
        "    bigram_freq[bigram[0]][bigram[1]] += 1\n",
        "\n",
        "for trigram in trigrams:\n",
        "    trigram_freq[(trigram[0], trigram[1])][trigram[2]] += 1\n",
        "\n",
        "# Function to predict the next word using bigrams\n",
        "def predict_next_bigram(current_word):\n",
        "    if current_word in bigram_freq:\n",
        "        #  selects one word from the list of possible next words,\n",
        "        # with the probability of each word being proportional to its frequency.\n",
        "        # This means that more frequent words are more likely to be selected.\n",
        "        next_word = random.choices(list(bigram_freq[current_word].keys()), list(bigram_freq[current_word].values()))[0]\n",
        "        return next_word\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to predict the next word using trigrams\n",
        "def predict_next_trigram(word_pair):\n",
        "    if word_pair in trigram_freq:\n",
        "        next_word = random.choices(list(trigram_freq[word_pair].keys()), list(trigram_freq[word_pair].values()))[0]\n",
        "        return next_word\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example predictions\n",
        "current_word = \"natural\"\n",
        "next_word_bigram = predict_next_bigram(current_word)\n",
        "print(f\"Bigram Prediction for '{current_word}':\", next_word_bigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trigram Prediction for '('natural', 'language')': processing\n"
          ]
        }
      ],
      "source": [
        "current_word = (\"natural\", \"language\")\n",
        "next_word_bigram = predict_next_trigram(current_word)\n",
        "print(f\"trigram Prediction for '{current_word}':\", next_word_bigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zftRhsTJcmIJ"
      },
      "source": [
        "## Statistical Language Models (HMM and MLE)\n",
        "\n",
        "First of a kind appoach where the aim was to find the implicit structure in the textual data. \n",
        "\n",
        "**Hidden Markov Models (HMM)**:\n",
        "- **Concept**: HMMs are used for sequence prediction where the system being modeled is assumed to follow a Markov process with hidden states.\n",
        "- **Components**: States (e.g., part-of-speech tags), Observations (words), Transition Probabilities (state to state), Emission Probabilities (state to word), and Initial Probabilities (start state).\n",
        "- **Application**: Used in part-of-speech tagging, where the hidden states are tags, and the observations are words.\n",
        "\n",
        "**Maximum Likelihood Estimation (MLE)**:\n",
        "- **Concept**: MLE is a statistical method for estimating the parameters of a model that maximize the likelihood of the observed data.\n",
        "- **Application**: In language models, MLE estimates the probabilities of sequences of words (e.g., bigram/trigram probabilities) by maximizing the likelihood of the observed word sequences in the training data.\n",
        "\n",
        "  ![hmm1](hmm1.png)\n",
        "\n",
        "  ![hmm2](hmm2.png)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3CeoZTbEQYa",
        "outputId": "ab691db3-2fb5-404b-d569-3f41d70b7be6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "\n",
        "# Prepare training data (simplified for illustration purposes)\n",
        "train_data = [\n",
        "    [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')],\n",
        "    [('The', 'DT'), ('ultimate', 'JJ'), ('goal', 'NN'), ('of', 'IN'), ('NLP', 'NNP'), ('is', 'VBZ'), ('to', 'TO'), ('enable', 'VB'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), (',', ','), ('interpret', 'VB'), (',', ','), ('and', 'CC'), ('generate', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')],\n",
        "    [('Over', 'IN'), ('the', 'DT'), ('years', 'NNS'), (',', ','), ('NLP', 'NNP'), ('has', 'VBZ'), ('seen', 'VBN'), ('significant', 'JJ'), ('advancements', 'NNS'), (',', ','), ('driven', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('development', 'NN'), ('of', 'IN'), ('sophisticated', 'JJ'), ('algorithms', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('availability', 'NN'), ('of', 'IN'), ('large', 'JJ'), ('datasets', 'NNS'), ('.', '.')],\n",
        "    [('Techniques', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('deep', 'JJ'), ('learning', 'NN'), (',', ','), ('and', 'CC'), ('neural', 'JJ'), ('networks', 'NNS'), ('have', 'VBP'), ('transformed', 'VBN'), ('NLP', 'NNP'), (',', ','), ('enabling', 'VBG'), ('applications', 'NNS'), ('like', 'IN'), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('machine', 'NN'), ('translation', 'NN'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('and', 'CC'), ('conversational', 'JJ'), ('agents', 'NNS'), ('.', '.')],\n",
        "    [('These', 'DT'), ('technologies', 'NNS'), ('have', 'VBP'), ('not', 'RB'), ('only', 'RB'), ('improved', 'VBN'), ('the', 'DT'), ('accuracy', 'NN'), ('and', 'CC'), ('efficiency', 'NN'), ('of', 'IN'), ('language', 'NN'), ('processing', 'NN'), ('tasks', 'NNS'), ('but', 'CC'), ('have', 'VBP'), ('also', 'RB'), ('expanded', 'VBN'), ('the', 'DT'), ('range', 'NN'), ('of', 'IN'), ('possible', 'JJ'), ('applications', 'NNS'), (',', ','), ('making', 'VBG'), ('human-computer', 'JJ'), ('interaction', 'NN'), ('more', 'RBR'), ('natural', 'JJ'), ('and', 'CC'), ('intuitive', 'JJ'), ('.', '.')]\n",
        "]\n",
        "\n",
        "# Train HMM\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(train_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural language processing language processing language processing language processing language\n"
          ]
        }
      ],
      "source": [
        "# Function to generate the next word using HMM with consideration to avoid repetition\n",
        "def generate_next_word(tagger, context, prev_word=None):\n",
        "    tagged_context = tagger.tag(context.split())\n",
        "    last_word = tagged_context[-1][0]\n",
        "    last_tag = tagged_context[-1][1]\n",
        "\n",
        "    # Get the next state (tag)\n",
        "    next_tag = max(tagger._transitions[last_tag].samples(), key=lambda tag: tagger._transitions[last_tag].prob(tag))\n",
        "\n",
        "    # Get the next word from the emission probabilities, avoiding repetition\n",
        "    word_probs = {word: tagger._outputs[next_tag].prob(word) for word in tagger._outputs[next_tag].samples()}\n",
        "    if prev_word and prev_word in word_probs:\n",
        "        del word_probs[prev_word]\n",
        "    \n",
        "    next_word = max(word_probs, key=word_probs.get)\n",
        "\n",
        "    return next_word\n",
        "\n",
        "# Function to generate a sentence of n words\n",
        "def generate_sentence(tagger, n):\n",
        "    context = 'Natural'\n",
        "    sentence = [context]\n",
        "\n",
        "    for _ in range(n - 1):\n",
        "        next_word = generate_next_word(tagger, context, sentence[-1])\n",
        "        sentence.append(next_word)\n",
        "        context += ' ' + next_word\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Example usage\n",
        "sentence = generate_sentence(tagger, 10)\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQvg4sTEcvhC"
      },
      "source": [
        "## The Rise of Machine Learning\n",
        "\n",
        "**Concept**:\n",
        "- **Shift from Rule-based to Data-driven Approaches**: Early NLP systems used hand-crafted rules, which were rigid and limited. The rise of machine learning introduced data-driven approaches that could learn patterns from large datasets.\n",
        "- **Naive Bayes**: A probabilistic classifier that applies Bayes' theorem with strong (naive) independence assumptions between features. Commonly used for text classification tasks like spam detection.\n",
        "- **Logistic Regression**: A statistical model used for binary classification tasks. It models the probability that a given input belongs to a particular category.\n",
        "- Example: Adding labels to customer issues, tshirt-sizes to developer issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKYShalOEXAk",
        "outputId": "2eba2ff2-c0c9-4596-99ff-9d575ee620d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: [1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Example data\n",
        "texts = [\"I love natural language processing\", \"Natural language processing is fun\", \"I dislike processing errors\"]\n",
        "labels = [1, 1, 0]  # 1: positive, 0: negative\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Train Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "# Predict example\n",
        "example = vectorizer.transform([\"I I love natural language processing\"])\n",
        "prediction = model.predict(example)\n",
        "print(\"Prediction:\", prediction)\n",
        "\n",
        "model.predict(vectorizer.transform([texts[2]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZjsrje6c12S"
      },
      "source": [
        "## Neural Networks in NLP\n",
        "\n",
        "**Feedforward Neural Networks**:\n",
        "- **Concept**: Basic neural network architecture where information moves in one direction from input to output.\n",
        "- **Limitation**: Cannot capture sequential information in text data.\n",
        "\n",
        "## Word Embeddings\n",
        "![word2vec](w2v.png)\n",
        "**Concept**:\n",
        "- **Word Embeddings**: These are dense vector representations of words that capture semantic meanings. Words with similar meanings have similar vector representations.\n",
        "- **Word2Vec**: Predicts the context of a word (Skip-gram) or words in context (CBOW). It captures the semantic relationships between words.\n",
        "- **GloVe**: Global Vectors for Word Representation, which combines global word-word co-occurrence statistics.\n",
        "- **FastText**: Extends Word2Vec by considering subword information, which helps in handling out-of-vocabulary words.\n",
        "\n",
        "**Application**:\n",
        "- Top-k similar products (Spotify, Amazon, Youtube recommender systems)\n",
        "- Information retrieval\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elqdnegPEmAB",
        "outputId": "c7148098-217b-4523-c3f7-b96744b4340f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec Embedding for 'natural': [-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
            "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
            " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
            " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
            "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
            "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
            "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
            " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
            "  0.00302193  0.00358008]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example data\n",
        "sentences = [[\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
        "             [\"natural\", \"language\", \"processing\", \"is\", \"fun\"]]\n",
        "\n",
        "# Train Word2Vec\n",
        "model = Word2Vec(sentences, vector_size=50, min_count=1)\n",
        "\n",
        "# Get embeddings\n",
        "vector = model.wv['natural']\n",
        "print(\"Word2Vec Embedding for 'natural':\", vector)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4RYyRTHdOBr"
      },
      "source": [
        "**Recurrent Neural Networks (RNNs)**:\n",
        "![](rnn.png)\n",
        "- **Concept**: Designed to handle sequential data by maintaining a hidden state that captures information about previous elements in the sequence.\n",
        "- **Limitation**: Struggles with long-term dependencies due to vanishing gradient problems.\n",
        "\n",
        "**Long Short-Term Memory (LSTM)**:\n",
        "![](lstm.png)\n",
        "- **Concept**: An improved version of RNNs designed to capture long-term dependencies by using gates to control the flow of information. There are input, output, and forget gates. The gates allow for the gradients to flow.  \n",
        "- **Application**: Used in tasks like language modeling, machine translation, and speech recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "848MLOuRFAMJ",
        "outputId": "0d525199-d837-4ded-fb0e-65462cfc6141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.7109162211418152\n",
            "Epoch 2/10, Loss: 0.6827574968338013\n",
            "Epoch 3/10, Loss: 0.6552144885063171\n",
            "Epoch 4/10, Loss: 0.6282566785812378\n",
            "Epoch 5/10, Loss: 0.6018567681312561\n",
            "Epoch 6/10, Loss: 0.5759904384613037\n",
            "Epoch 7/10, Loss: 0.550626277923584\n",
            "Epoch 8/10, Loss: 0.5257291793823242\n",
            "Epoch 9/10, Loss: 0.501266598701477\n",
            "Epoch 10/10, Loss: 0.47721171379089355\n",
            "LSTM Prediction: 0.6247726082801819\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example data\n",
        "texts = [\"I love natural language processing\", \"natural language processing is fun\"]\n",
        "labels = [1, 1]  # Example binary labels\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Pad sequences\n",
        "data = pad_sequences(sequences)\n",
        "labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDataset\n",
        "data_tensor = torch.tensor(data, dtype=torch.long)\n",
        "dataset = TensorDataset(data_tensor, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, (hn, cn) = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(word_index) + 1\n",
        "embed_size = 50\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (for illustration, normally would train for more epochs)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Example prediction\n",
        "with torch.no_grad():\n",
        "    example_sequence = torch.tensor(pad_sequences(tokenizer.texts_to_sequences([\"I love natural language processing\"]), maxlen=data.shape[1]), dtype=torch.long)\n",
        "    prediction = model(example_sequence)\n",
        "    print(\"LSTM Prediction:\", prediction.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7rAt4C3dWk_"
      },
      "source": [
        "## The Transformer Revolution\n",
        "\n",
        "![t](tf.png)\n",
        "\n",
        "**Concept**:\n",
        "- **Transformer Model**: Introduced by Vaswani et al., it relies on a self-attention mechanism to process entire sequences in parallel, unlike RNNs which process sequentially.\n",
        "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different words in a sequence, capturing long-range dependencies more effectively.\n",
        "- **Impact**: Revolutionized NLP by enabling the training of much larger models on larger datasets, leading to significant improvements in performance on various NLP tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOAmmrQVFVNd",
        "outputId": "342b1de9-00b0-4f6d-fdcc-d7d6eaa814ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: I love natural language processing, and that sounds like an interesting topic. As another good example, I\n"
          ]
        }
      ],
      "source": [
        "# Note: Transformers require a more complex setup and often large datasets.\n",
        "# Here we use a pre-trained model from Hugging Face for simplicity.\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Generate text\n",
        "result = generator(\"I love natural language processing\", max_length=100)\n",
        "print(\"Generated Text:\", result[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7PzHNrlddwl"
      },
      "source": [
        "## BERT and Pre-trained Models\n",
        "\n",
        "**Concept**:\n",
        "- **BERT (Bidirectional Encoder Representations from Transformers)**: Pre-trained on large corpora and fine-tuned for specific tasks. It uses a bidirectional approach to understand context from both directions.\n",
        "- **Pre-training and Fine-tuning**: Pre-training on a large dataset and then fine-tuning on a smaller, task-specific dataset.\n",
        "- **Applications**: Question answering, sentiment analysis, named entity recognition, and more.\n",
        "\n",
        "## GPT Series\n",
        "\n",
        "![](gpt.png)\n",
        "\n",
        "**Concept**:\n",
        "- **GPT (Generative Pre-trained Transformer)**: A series of models (GPT, GPT-2, GPT-3, GPT-4) that generate human-like text based on a given prompt.\n",
        "- **Architecture**: Uses a transformer architecture with a focus on generating text.\n",
        "- **Evolution**: Each subsequent model in the series has more parameters, leading to better performance and more coherent text generation.\n",
        "- **Applications**: Text completion, content creation, conversational agents, and more.\n",
        "\n",
        "### Summary:\n",
        "From early n-gram models to advanced transformer-based models, the evolution of language models in NLP has dramatically improved the ability to understand and generate human language. Each advancement brought better handling of context, dependencies, and semantic meaning, leading to more accurate and versatile applications in various NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "7cbd2a1719cf41c7bd531fcd0e619826",
            "fb1fd90cfa39473b9f6257f370e4bd87",
            "143bfc2d7bb8405cab349b8197c8e978",
            "b56b34c09f7e45b592218c472702b02a",
            "793bf6924c0b495dbacdb38d2eba2927",
            "48032eadec7f4e3f8bf99f560a6b0907",
            "328fab1a69bb4f47b1fc19b3d26a52d2",
            "ea650071c87a4e10b3f1aef940e15f24",
            "0ebf95f08a754cc998474019fcb8b97a",
            "93fe81bb7c864ff38d8b952d3baa7b53",
            "6a210cbe5ea244b79594762020f8f62a"
          ]
        },
        "id": "wpdC_Rr16N0L",
        "outputId": "6d52bcb6-17d0-4b1f-923a-79f16cede499"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cbd2a1719cf41c7bd531fcd0e619826",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "model_name = \"openlm-research/open_llama_3b_v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "\n",
        "# Encode the input text\n",
        "input_text = \"I love natural language processing because\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Text:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMjkOIzHD3bo",
        "outputId": "4802c0db-0653-4c46-83ee-150494d4f570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: I love natural language processing because it is a great way to get a sense of what people are thinking. I have been working on a project to build a natural language processing system that can understand the meaning of a sentence.\n",
            "I have been working\n"
          ]
        }
      ],
      "source": [
        "print(\"Generated Text:\", generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ebf95f08a754cc998474019fcb8b97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "143bfc2d7bb8405cab349b8197c8e978": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea650071c87a4e10b3f1aef940e15f24",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ebf95f08a754cc998474019fcb8b97a",
            "value": 137
          }
        },
        "328fab1a69bb4f47b1fc19b3d26a52d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48032eadec7f4e3f8bf99f560a6b0907": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a210cbe5ea244b79594762020f8f62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "793bf6924c0b495dbacdb38d2eba2927": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cbd2a1719cf41c7bd531fcd0e619826": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb1fd90cfa39473b9f6257f370e4bd87",
              "IPY_MODEL_143bfc2d7bb8405cab349b8197c8e978",
              "IPY_MODEL_b56b34c09f7e45b592218c472702b02a"
            ],
            "layout": "IPY_MODEL_793bf6924c0b495dbacdb38d2eba2927"
          }
        },
        "93fe81bb7c864ff38d8b952d3baa7b53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56b34c09f7e45b592218c472702b02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93fe81bb7c864ff38d8b952d3baa7b53",
            "placeholder": "​",
            "style": "IPY_MODEL_6a210cbe5ea244b79594762020f8f62a",
            "value": " 137/137 [00:00&lt;00:00, 1.82kB/s]"
          }
        },
        "ea650071c87a4e10b3f1aef940e15f24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb1fd90cfa39473b9f6257f370e4bd87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48032eadec7f4e3f8bf99f560a6b0907",
            "placeholder": "​",
            "style": "IPY_MODEL_328fab1a69bb4f47b1fc19b3d26a52d2",
            "value": "generation_config.json: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
